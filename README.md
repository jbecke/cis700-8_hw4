# Fine-Tuning GPT-2 For Natural Language Generation
**Arun Kirubarajan & Jacob Beckerman**

## Part 1
> Compute the perplexity of test and validation sets according to GPT-2 without fine-tuning and according to GPT-2 with finetuning. Does perplexity go down after fine-tuning?

Perplexity without fine-tuning to presidential speeches: 
```
27.0704402923584 is the perplexity of /content/presidential_speeches_valid.txt according to gpt2-medium

23.510801315307617 is the perplexity of /content/presidential_speeches_test.txt according to gpt2-medium
```

Perplexity with fine-tuning to presidential speeches: 
```
19.99014663696289 is the perplexity of /content/presidential_speeches_valid.txt according to /content/drive/My Drive/finetuned_models/presidential_speeches/checkpoint-3000

18.390100479125977 is the perplexity of /content/presidential_speeches_test.txt according to /content/drive/My Drive/finetuned_models/presidential_speeches/checkpoint-3000
```

Thus, it is clear that fine-tuning to the test set does lower the perplexity.

> Generate at least 100 samples from GPT-2 with fine-tuning and 100 without fine-tuning. Compute the word (or token) overlap of the generated text with the text in the test set. Which set of generated sentences has more words in common with the text in the test set? Is this what you expected?

todo

> The provided code uses top-k with k=50 for generation. Experiment with different sampling strategies and observe how this impacts the quality and diversity of the generations. If youâ€™d like, implement a measure of text diversity such as self-BLEU or dist-1 (the number of unique generated words divided by the total number of generated words), and plot how it changes as you vary the value of either temperature, k, or p.

todo

## Part 2
> (same question as Part 1)

todo

> (same question as Part 2)

todo

> (same question as Part 3)

todo

> Describe your new dataset and how you collected it. Why do you think it is an interesting dataset? What are the sizes (in megabytes) of your train, validation, and test files? Copy some text from the dataset into your report to give us a sense of what it looks like.

todo

> Copy some of the text generated by the finetuned model into your report. Is it as good as you hoped? What are some problems you notice with it?

todo

> Did you have to tweak any of the flags passed to run_language_modeling.py to get finetuning working on your datasret? If so, which ones did you have to change?

todo
